{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpWm-TzoK894",
        "outputId": "f4b18a92-693a-495e-dda3-3ac76cea7aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing libraries...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing libraries...\")\n",
        "!uv pip install llama-cpp-python errant --quiet --prerelease allow\n",
        "!python -m spacy download en_core_web_sm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KsvTYZWzLCq-"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "import os\n",
        "import urllib.request\n",
        "import subprocess\n",
        "import tempfile\n",
        "import re\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# order matters: always substitute the longer N'T first, then the shorter endings\n",
        "CONTRACTIONS = [\"n't\", \"'ll\", \"'ve\", \"'re\", \"'d\", \"'m\", \"'s\"]\n",
        "\n",
        "# characters that carry a *leading* space in BEA tokenisation\n",
        "PUNCT = r\"\\.\\,\\!\\?\\:\\;\"\n",
        "\n",
        "def detokenize_bea(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert BEA‑style tokenisation to normal English prose.\n",
        "    \"\"\"\n",
        "    text = text.replace(\"’\", \"'\")\n",
        "    # 1. Join contractions\n",
        "    #   e.g. \"do n't\"  -> \"don't\"\n",
        "    #        \"she 'll\" -> \"she'll\"\n",
        "    # pattern:    <word> SP <contr>\n",
        "    for contr in CONTRACTIONS:\n",
        "        pattern = rf\"\\b(\\w+)\\s+{re.escape(contr)}\\b\"\n",
        "        repl    = rf\"\\1{contr}\"\n",
        "        text    = re.sub(pattern, repl, text)\n",
        "\n",
        "    # 2. Collapse space BEFORE punctuation  ( ... \"word .\" -> \"word.\" )\n",
        "    #  – only collapse if it’s a *single* normal blank, keep new‑lines\n",
        "    text = re.sub(r\" (?=[{}])\".format(PUNCT), \"\", text)\n",
        "\n",
        "    # 3. Clean spaces just inside brackets  : \"( word\" -> \"(word\"\n",
        "    text = re.sub(r\"\\(\\s+\", \"(\", text)\n",
        "    text = re.sub(r\"\\[\\s+\", \"[\", text)\n",
        "    text = re.sub(r\"\\s+\\)\", \")\", text)\n",
        "    text = re.sub(r\"\\s+\\]\", \"]\", text)\n",
        "\n",
        "    # 4. Collapse 2+ blanks into one\n",
        "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize_like_bea(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert normal English prose back to the peculiar BEA tokenisation.\n",
        "    (Not 100 % identical for every imaginable corner‑case, but round‑trips\n",
        "     correctly for the typical patterns in the dataset.)\n",
        "    \"\"\"\n",
        "    text = text.replace(\"’\", \"'\")\n",
        "    # 1. Split contractions      e.g. \"don't\" -> \"do n't\"\n",
        "    # n't must be done first because \"won't\" → \"wo n't\", not \"wo n' t\"\n",
        "    text = re.sub(r\"\\b(\\w+)n't\\b\",  r\"\\1 n't\", text)\n",
        "\n",
        "    # remaining endings\n",
        "    for contr in [\"'ll\", \"'ve\", \"'re\", \"'d\", \"'m\", \"'s\"]:\n",
        "        pattern = rf\"\\b(\\w+){re.escape(contr)}\\b\"\n",
        "        repl    = rf\"\\1 {contr}\"\n",
        "        text    = re.sub(pattern, repl, text)\n",
        "\n",
        "    # 2. Add a space *before* the main punctuation marks\n",
        "    # We first remove any existing spaces, then add exactly one.\n",
        "    text = re.sub(r\"\\s*([{}])\".format(PUNCT), r\" \\1\", text)\n",
        "\n",
        "    # 3. Remove extra double spaces that might have appeared\n",
        "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
        "\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "jDD9_oCo-Cd8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYSFuGEILFxQ",
        "outputId": "1649f9c6-3a97-4881-dc77-7badedf3412b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/grammarly/pillars-of-gec/main/data/evaluation_sets/bea-dev.txt...\n",
            "File downloaded to: data/bea-dev.txt\n",
            "Downloading https://raw.githubusercontent.com/grammarly/pillars-of-gec/refs/heads/main/data/evaluation_sets/bea-dev.m2...\n",
            "File downloaded to: data/bea-dev.m2\n",
            "Read 4384 total sentences from data/bea-dev.txt\n",
            "Using a subset of 25 sentences for evaluation.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Configuration ---\n",
        "MAX_EVAL_SAMPLES = 25 # Set to None to evaluate on the full dataset\n",
        "LOG_SAMPLE_OUTPUTS = 10 # Number of original/corrected pairs to print per model\n",
        "# GEC_PROOMPT = \"Fix grammaticality in this sentence:\"\n",
        "GEC_PROOMPT = \"\"\n",
        "\n",
        "# --- Loading data ---\n",
        "\n",
        "# Raw URL of the file\n",
        "bea_dev = \"https://raw.githubusercontent.com/grammarly/pillars-of-gec/main/data/evaluation_sets/bea-dev.txt\"\n",
        "bea_m2 = \"https://raw.githubusercontent.com/grammarly/pillars-of-gec/refs/heads/main/data/evaluation_sets/bea-dev.m2\"\n",
        "\n",
        "folder = \"data\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "dev_file_path = os.path.join(folder, \"bea-dev.txt\")\n",
        "if not os.path.exists(dev_file_path):\n",
        "    print(f\"Downloading {bea_dev}...\")\n",
        "    urllib.request.urlretrieve(bea_dev, dev_file_path)\n",
        "    print(f\"File downloaded to: {dev_file_path}\")\n",
        "else:\n",
        "    print(f\"Using existing file: {dev_file_path}\")\n",
        "\n",
        "\n",
        "m2_file_path = os.path.join(folder, \"bea-dev.m2\")\n",
        "if not os.path.exists(m2_file_path):\n",
        "    print(f\"Downloading {bea_m2}...\")\n",
        "    urllib.request.urlretrieve(bea_m2, m2_file_path)\n",
        "    print(f\"File downloaded to: {m2_file_path}\")\n",
        "else:\n",
        "     print(f\"Using existing file: {m2_file_path}\")\n",
        "\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def read_sentences(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "def create_subset_m2(input_m2_path, output_m2_path, num_sentences):\n",
        "    \"\"\"Reads an M2 file and writes the first num_sentences entries to a new file.\"\"\"\n",
        "    print(f\"Creating subset M2 file ({num_sentences} sentences) at: {output_m2_path}\")\n",
        "    sentence_count = 0\n",
        "    with open(input_m2_path, 'r', encoding='utf-8') as infile, \\\n",
        "         open(output_m2_path, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            # Ensure line is not empty before checking prefix\n",
        "            if line.strip() and line.startswith('S '):\n",
        "                sentence_count += 1\n",
        "                if sentence_count > num_sentences:\n",
        "                    break\n",
        "            # Check sentence_count before writing any line related to the current sentence block\n",
        "            if sentence_count <= num_sentences and sentence_count > 0: # Only write if we are within the limit and started processing sentences\n",
        "                 outfile.write(line)\n",
        "            # Handle edge case where the loop might end before writing the last newline of the final sentence block\n",
        "        if sentence_count <= num_sentences and sentence_count > 0:\n",
        "             outfile.write('\\n') # Ensure last entry ends properly if needed\n",
        "\n",
        "    print(f\"Finished creating subset M2 file.\")\n",
        "\n",
        "\n",
        "# --- Read Original Sentences (BEA Format) ---\n",
        "all_original_sentences_bea = read_sentences(dev_file_path)\n",
        "print(f\"Read {len(all_original_sentences_bea)} total sentences from {dev_file_path}\")\n",
        "\n",
        "# Select subset for evaluation\n",
        "if MAX_EVAL_SAMPLES is not None and MAX_EVAL_SAMPLES < len(all_original_sentences_bea):\n",
        "    original_sentences_bea = all_original_sentences_bea[:MAX_EVAL_SAMPLES]\n",
        "    print(f\"Using a subset of {len(original_sentences_bea)} sentences for evaluation.\")\n",
        "else:\n",
        "    original_sentences_bea = all_original_sentences_bea\n",
        "    print(f\"Using the full dataset of {len(original_sentences_bea)} sentences for evaluation.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ctSlvVFCLLFD"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LLM:\n",
        "    def __init__(self, repo_id: str, filename: str, verbose: bool = False, n_gpu_layers=-1, gec_prompt:str = GEC_PROOMPT, **kwargs):\n",
        "        self.model_name = repo_id.split('/')[-1] # Extract model name for reporting\n",
        "        print(f\"Initializing {self.model_name}...\")\n",
        "        self.gec_prompt = gec_prompt\n",
        "        print(f\"Using gec prompt: {gec_prompt}\")\n",
        "        self.model = Llama.from_pretrained(\n",
        "            repo_id=repo_id,\n",
        "            filename=filename,\n",
        "            verbose=verbose,\n",
        "            n_gpu_layers=-1,\n",
        "            **kwargs,\n",
        "        )\n",
        "        print(f\"{self.model_name} initialized.\")\n",
        "\n",
        "    # Takes NATURAL language prompts, returns NATURAL language results\n",
        "    def __call__(self, prompts: list[str], system_prompt: str = \"Correct any grammatical errors in the provided sentence. Only output the corrected sentence.\", temperature: float = 0.1, max_tokens=150, **kwargs):\n",
        "        results = []\n",
        "        total_prompts = len(prompts)\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            # Add progress indicator\n",
        "            print(f\"  Processing prompt {i+1}/{total_prompts}...\", end='\\r')\n",
        "            try:\n",
        "                response = self.model.create_chat_completion(\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": f\"{self.gec_prompt}+ {prompt}\"}\n",
        "                    ],\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                    **kwargs\n",
        "                )\n",
        "                # Handle potential empty responses or formatting issues\n",
        "                if response['choices'] and 'message' in response['choices'][0] and 'content' in response['choices'][0]['message']:\n",
        "                    correction = response['choices'][0]['message']['content'].strip()\n",
        "                    # Basic check for empty string, could be more robust\n",
        "                    if correction:\n",
        "                         results.append(correction)\n",
        "                    else:\n",
        "                        print(f\"\\nWarning: Model generated empty output for prompt: {prompt[:80]}...\") # Log snippet\n",
        "                        results.append(prompt) # Use original sentence as fallback\n",
        "                else:\n",
        "                    print(f\"\\nWarning: Unexpected response structure or no choices for prompt: {prompt[:80]}...\")\n",
        "                    results.append(prompt) # Fallback to original\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during model inference for prompt: {prompt[:80]}...\\n{e}\")\n",
        "                # Fallback to original sentence on error to avoid crashing evaluation\n",
        "                results.append(prompt)\n",
        "        print(f\"\\n  Finished processing {total_prompts} prompts.\") # Newline after progress\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wden5wF5LOVN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "8afc733442c64672aed1542bf205ead4",
            "89ffe66699ce4b8c84f7c85253ce5801",
            "af30be4b368d41228b1a6ff51717bc23",
            "a5b7a19ad11e4ce3980a3b743051f98b",
            "f536a0b0dd5e49e1bec951de133f82b8",
            "b8f765546c334ead8d8a76ce69dc8334",
            "7eba7751b91c4562a9b02dc9ec5caa32",
            "eea0c9e8f6d5432e8535b52ee9ed84cf",
            "0006db3929c34f83aed7a93f89511eaf",
            "88ef95ac10f44d2f9bfd190ebd95c71a",
            "5de55033ce4841a1a6f885a72520683f",
            "1f0e2595516d4151947b68f26148e9f6",
            "8f1aa35321784b958668845ad974c41b",
            "4047c897c50243bd8190f858dc2ab369",
            "5e78473f15f547549e43372cba12b528",
            "e364935ffc8c4f6086e1203643b3de85",
            "32bada8fa6d74de7bd7f09216868cab1",
            "04f428e97fa84541acd5684239708041",
            "f708042ba4f34992a377ea678d7283f4",
            "1079de77ecb74dbca5e0ba54aff3b414",
            "8fa6a48e43ca46c2ad594b9f9b0c7d11",
            "7bcbfa0f22cc4258af9e12e95b823627"
          ]
        },
        "outputId": "b693cc09-05e7-4e72-d2c3-4690d1e4715d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing gemma-3-4b-it-GGUF...\n",
            "Using gec prompt: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gemma-3-4b-it-Q4_K_M.gguf:   0%|          | 0.00/2.49G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8afc733442c64672aed1542bf205ead4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gemma-3-4b-it-GGUF initialized.\n",
            "Initializing nomodit-4b-merged-v0-Q4_K_M-GGUF...\n",
            "Using gec prompt: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "nomodit-4b-merged-v0-q4_k_m.gguf:   0%|          | 0.00/2.49G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f0e2595516d4151947b68f26148e9f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nomodit-4b-merged-v0-Q4_K_M-GGUF initialized.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Model Initialization ---\n",
        "# Using Q4_K_M for potentially faster eval runs\n",
        "\n",
        "llm_instances = {\n",
        "    \"gemma-3-4b-it\": LLM(\"unsloth/gemma-3-4b-it-GGUF\", filename=\"*Q4_K_M.gguf\", verbose=False),\n",
        "    \"nomodit-4b\": LLM(\"muzzz/nomodit-4b-merged-v0-Q4_K_M-GGUF\", filename=\"*q4_k_m.gguf\", verbose=False), # Assuming common params are ok\n",
        "    # \"phi-4-mini-instruct\": LLM(\"unsloth/Phi-4-mini-instruct-GGUF\", filename=\"*Q4_K_M.gguf\", verbose=False),\n",
        "    # \"phi-4\": LLM(\"unsloth/phi-4-GGUF\", filename=\"*Q4_K_M.gguf\", verbose=False),\n",
        "    # \"llama-3.1-8B-Instruct\": LLM(\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\", filename=\"*Q4_K_M.gguf\", verbose=False),\n",
        "    # \"llama-3.2-3B-Instruct\": LLM(\"unsloth/Llama-3.2-3B-Instruct-GGUF\", filename=\"*Q4_K_M.gguf\", verbose=False),\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vZJ3KAmzLYNy"
      },
      "outputs": [],
      "source": [
        "# --- Benchmarking ---\n",
        "\n",
        "models_to_benchmark_names = [\n",
        "    \"gemma-3-4b-it\",\n",
        "    \"nomodit-4b\"\n",
        "    # \"phi-4-mini-instruct\",\n",
        "    # \"phi-4\",\n",
        "    # \"llama-3.1-8B-Instruct\",\n",
        "    # \"llama-3.2-3B-Instruct\",\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Regex to capture P, R, F0.5 from errant_compare output\n",
        "# Updated regex to handle the table format output\n",
        "score_pattern = re.compile(\n",
        "    r\"=========== Span-Based Correction ============\\n\"  # Match header\n",
        "    r\"TP\\s+FP\\s+FN\\s+Prec\\s+Rec\\s+F0\\.5\\n\"             # Match table header row\n",
        "    r\"\\d+\\s+\\d+\\s+\\d+\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\" # Capture Prec, Rec, F0.5 from data row\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KiURHm0HLZ11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49da6a08-f214-4def-a64b-ea78d0faee9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Benchmark ---\n",
            "Detokenizing 25 sentences for LLM input...\n",
            "Detokenization complete.\n",
            "Creating subset M2 file (25 sentences) at: /tmp/tmpni2jjd6g.m2\n",
            "Finished creating subset M2 file.\n",
            "Using subset reference M2: /tmp/tmpni2jjd6g.m2\n",
            "\n",
            "===== Evaluating model: gemma-3-4b-it-GGUF =====\n",
            "Created temp orig (BEA format): /tmp/tmp9yoiz_8j.txt\n",
            "Generating corrections for 25 sentences...\n",
            "\n",
            "  Finished processing 25 prompts.\n",
            "Generation took: 355.55 seconds\n",
            "Retokenizing LLM output to BEA format...\n",
            "Retokenization complete.\n",
            "\n",
            "--- Sample Outputs for gemma-3-4b-it-GGUF (First 10) ---\n",
            "Orig (BEA)  [1]: It 's difficult answer at the question \" what are you going to do in the future ? \" if the only one who has to know it is in two minds .\n",
            "Input (Nat) [1]: It's difficult answer at the question \" what are you going to do in the future? \" if the only one who has to know it is in two minds.\n",
            "Output (Nat)[1]: It’s difficult to answer the question “What are you going to do in the future?” if the only one who has to know it is in two minds.\n",
            "Corr (BEA)  [1]: It’s difficult to answer the question “What are you going to do in the future ?” if the only one who has to know it is in two minds .\n",
            "---\n",
            "Orig (BEA)  [2]: When I was younger I used to say that I wanted to be a teacher , a saleswoman and even a butcher .. I do n't know why .\n",
            "Input (Nat) [2]: When I was younger I used to say that I wanted to be a teacher, a saleswoman and even a butcher.. I don't know why.\n",
            "Output (Nat)[2]: When I was younger, I used to say that I wanted to be a teacher, a salesperson, and even a butcher. I don't know why.\n",
            "Corr (BEA)  [2]: When I was younger , I used to say that I wanted to be a teacher , a salesperson , and even a butcher . I do n't know why .\n",
            "---\n",
            "Orig (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "Input (Nat) [3]: I would like to study Psychology because one day I would open my own psychology office and help people.\n",
            "Output (Nat)[3]: I would like to study Psychology because one day I would open my own psychology office and help people.\n",
            "Corr (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "---\n",
            "Orig (BEA)  [4]: It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it easier .\n",
            "Input (Nat) [4]: It's difficult because I'll have to study hard and a lot, but I think that if you like a subject, you'll study it easier.\n",
            "Output (Nat)[4]: It’s difficult because I’ll have to study hard and a lot, but I think that if you like a subject, you’ll study it more easily.\n",
            "Corr (BEA)  [4]: It’s difficult because I’ll have to study hard and a lot , but I think that if you like a subject , you’ll study it more easily .\n",
            "---\n",
            "Orig (BEA)  [5]: Maybe I 'll change my mind , maybe not .\n",
            "Input (Nat) [5]: Maybe I'll change my mind, maybe not.\n",
            "Output (Nat)[5]: Maybe I’ll change my mind, maybe not.\n",
            "Corr (BEA)  [5]: Maybe I’ll change my mind , maybe not .\n",
            "---\n",
            "Orig (BEA)  [6]: I think that the public transport will always be in the future .\n",
            "Input (Nat) [6]: I think that the public transport will always be in the future.\n",
            "Output (Nat)[6]: I think that public transport will always be important in the future.\n",
            "Corr (BEA)  [6]: I think that public transport will always be important in the future .\n",
            "---\n",
            "Orig (BEA)  [7]: The rich people will buy a car but the poor people always need to use a bus or taxi .\n",
            "Input (Nat) [7]: The rich people will buy a car but the poor people always need to use a bus or taxi.\n",
            "Output (Nat)[7]: The rich people will buy a car, but the poor people always need to use a bus or a taxi.\n",
            "Corr (BEA)  [7]: The rich people will buy a car , but the poor people always need to use a bus or a taxi .\n",
            "---\n",
            "Orig (BEA)  [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car .\n",
            "Input (Nat) [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car.\n",
            "Output (Nat)[8]: I consider it more convenient to drive a car because you can carry more things in your own car than when traveling by train.\n",
            "Corr (BEA)  [8]: I consider it more convenient to drive a car because you can carry more things in your own car than when traveling by train .\n",
            "---\n",
            "Orig (BEA)  [9]: Also , you 'll meet friendly people who usually ask to you something to be friends and change your telephone number .\n",
            "Input (Nat) [9]: Also, you'll meet friendly people who usually ask to you something to be friends and change your telephone number.\n",
            "Output (Nat)[9]: Also, you’ll meet friendly people who usually ask you to befriend them and change your telephone number.\n",
            "Corr (BEA)  [9]: Also , you’ll meet friendly people who usually ask you to befriend them and change your telephone number .\n",
            "---\n",
            "Orig (BEA)  [10]: In my experience when I did n't have a car I used to use the bus to go to the school and go back to my house .\n",
            "Input (Nat) [10]: In my experience when I didn't have a car I used to use the bus to go to the school and go back to my house.\n",
            "Output (Nat)[10]: In my experience, when I didn't have a car, I used to take the bus to school and back home.\n",
            "Corr (BEA)  [10]: In my experience , when I did n't have a car , I used to take the bus to school and back home .\n",
            "---\n",
            "\n",
            "\n",
            "Created temp corrected (BEA format): /tmp/tmpbrwaa2r2.cor.txt\n",
            "Running errant_parallel... Orig: /tmp/tmp9yoiz_8j.txt, Cor: /tmp/tmpbrwaa2r2.cor.txt, Out: /tmp/tmp3rf7fir5.hyp.m2\n",
            "Running errant_compare... Hyp: /tmp/tmp3rf7fir5.hyp.m2, Ref: /tmp/tmpni2jjd6g.m2\n",
            "Evaluation took: 13.40 seconds\n",
            "Scores for gemma-3-4b-it-GGUF (on 25 samples): P=0.3289, R=0.4032, F0.5=0.3415\n",
            "\n",
            "===== Evaluating model: nomodit-4b-merged-v0-Q4_K_M-GGUF =====\n",
            "Created temp orig (BEA format): /tmp/tmp32ae8ffr.txt\n",
            "Generating corrections for 25 sentences...\n",
            "\n",
            "  Finished processing 25 prompts.\n",
            "Generation took: 355.83 seconds\n",
            "Retokenizing LLM output to BEA format...\n",
            "Retokenization complete.\n",
            "\n",
            "--- Sample Outputs for nomodit-4b-merged-v0-Q4_K_M-GGUF (First 10) ---\n",
            "Orig (BEA)  [1]: It 's difficult answer at the question \" what are you going to do in the future ? \" if the only one who has to know it is in two minds .\n",
            "Input (Nat) [1]: It's difficult answer at the question \" what are you going to do in the future? \" if the only one who has to know it is in two minds.\n",
            "Output (Nat)[1]: It's a difficult answer to the question \"what are you going to do in the future?\" if the only one who has to know it is in two minds.\n",
            "Corr (BEA)  [1]: It 's a difficult answer to the question \"what are you going to do in the future ?\" if the only one who has to know it is in two minds .\n",
            "---\n",
            "Orig (BEA)  [2]: When I was younger I used to say that I wanted to be a teacher , a saleswoman and even a butcher .. I do n't know why .\n",
            "Input (Nat) [2]: When I was younger I used to say that I wanted to be a teacher, a saleswoman and even a butcher.. I don't know why.\n",
            "Output (Nat)[2]: When I was younger, I used to say that I wanted to be a teacher, a saleswoman, and even a butcher. I don't know why.\n",
            "Corr (BEA)  [2]: When I was younger , I used to say that I wanted to be a teacher , a saleswoman , and even a butcher . I do n't know why .\n",
            "---\n",
            "Orig (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "Input (Nat) [3]: I would like to study Psychology because one day I would open my own psychology office and help people.\n",
            "Output (Nat)[3]: I would like to study Psychology because one day I would open my own psychology practice and help people.\n",
            "Corr (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology practice and help people .\n",
            "---\n",
            "Orig (BEA)  [4]: It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it easier .\n",
            "Input (Nat) [4]: It's difficult because I'll have to study hard and a lot, but I think that if you like a subject, you'll study it easier.\n",
            "Output (Nat)[4]: It's difficult because I'll have to study hard and a lot, but I think that if you like a subject, you'll study it easier.\n",
            "Corr (BEA)  [4]: It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it easier .\n",
            "---\n",
            "Orig (BEA)  [5]: Maybe I 'll change my mind , maybe not .\n",
            "Input (Nat) [5]: Maybe I'll change my mind, maybe not.\n",
            "Output (Nat)[5]: Maybe I'll change my mind, maybe I won't.\n",
            "Corr (BEA)  [5]: Maybe I 'll change my mind , maybe I wo n't .\n",
            "---\n",
            "Orig (BEA)  [6]: I think that the public transport will always be in the future .\n",
            "Input (Nat) [6]: I think that the public transport will always be in the future.\n",
            "Output (Nat)[6]: I think that public transport will always be in the future.\n",
            "Corr (BEA)  [6]: I think that public transport will always be in the future .\n",
            "---\n",
            "Orig (BEA)  [7]: The rich people will buy a car but the poor people always need to use a bus or taxi .\n",
            "Input (Nat) [7]: The rich people will buy a car but the poor people always need to use a bus or taxi.\n",
            "Output (Nat)[7]: The rich people will buy a car, but the poor people always need to use a bus or taxi.\n",
            "Corr (BEA)  [7]: The rich people will buy a car , but the poor people always need to use a bus or taxi .\n",
            "---\n",
            "Orig (BEA)  [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car .\n",
            "Input (Nat) [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car.\n",
            "Output (Nat)[8]: I consider that it is more convenient to drive a car because you carry on more things in your own car than travelling by car.\n",
            "Corr (BEA)  [8]: I consider that it is more convenient to drive a car because you carry on more things in your own car than travelling by car .\n",
            "---\n",
            "Orig (BEA)  [9]: Also , you 'll meet friendly people who usually ask to you something to be friends and change your telephone number .\n",
            "Input (Nat) [9]: Also, you'll meet friendly people who usually ask to you something to be friends and change your telephone number.\n",
            "Output (Nat)[9]: Also, you'll meet friendly people who usually ask you something to be friends and change your telephone number.\n",
            "Corr (BEA)  [9]: Also , you 'll meet friendly people who usually ask you something to be friends and change your telephone number .\n",
            "---\n",
            "Orig (BEA)  [10]: In my experience when I did n't have a car I used to use the bus to go to the school and go back to my house .\n",
            "Input (Nat) [10]: In my experience when I didn't have a car I used to use the bus to go to the school and go back to my house.\n",
            "Output (Nat)[10]: In my experience, when I didn't have a car, I used to use the bus to go to school and go back to my house.\n",
            "Corr (BEA)  [10]: In my experience , when I did n't have a car , I used to use the bus to go to school and go back to my house .\n",
            "---\n",
            "\n",
            "\n",
            "Created temp corrected (BEA format): /tmp/tmpjz36pbwb.cor.txt\n",
            "Running errant_parallel... Orig: /tmp/tmp32ae8ffr.txt, Cor: /tmp/tmpjz36pbwb.cor.txt, Out: /tmp/tmpsv6u601f.hyp.m2\n",
            "Running errant_compare... Hyp: /tmp/tmpsv6u601f.hyp.m2, Ref: /tmp/tmpni2jjd6g.m2\n",
            "Evaluation took: 9.14 seconds\n",
            "Scores for nomodit-4b-merged-v0-Q4_K_M-GGUF (on 25 samples): P=0.4651, R=0.3226, F0.5=0.4274\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n--- Starting Benchmark ---\")\n",
        "\n",
        "# --- Prepare Input Data --- #\n",
        "# Detokenize the input sentences ONCE before the loop\n",
        "print(f\"Detokenizing {len(original_sentences_bea)} sentences for LLM input...\")\n",
        "natural_input_sentences = [detokenize_bea(s) for s in original_sentences_bea]\n",
        "print(\"Detokenization complete.\")\n",
        "\n",
        "# --- Create Reference M2 Subset (if needed) --- #\n",
        "reference_m2_to_use = m2_file_path\n",
        "temp_ref_m2_subset_path = None # Initialize path variable\n",
        "\n",
        "if MAX_EVAL_SAMPLES is not None and MAX_EVAL_SAMPLES < len(all_original_sentences_bea):\n",
        "    # Create a temporary file path that persists until explicitly deleted\n",
        "    temp_ref_m2_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".m2\", encoding='utf-8')\n",
        "    temp_ref_m2_subset_path = temp_ref_m2_file.name\n",
        "    temp_ref_m2_file.close() # Close the file handle immediately, but keep the file\n",
        "    create_subset_m2(m2_file_path, temp_ref_m2_subset_path, len(original_sentences_bea))\n",
        "    reference_m2_to_use = temp_ref_m2_subset_path\n",
        "    print(f\"Using subset reference M2: {reference_m2_to_use}\")\n",
        "else:\n",
        "    print(f\"Using full reference M2: {reference_m2_to_use}\")\n",
        "\n",
        "\n",
        "for model_key in models_to_benchmark_names:\n",
        "    llm = llm_instances.get(model_key)\n",
        "    if not llm:\n",
        "        print(f\"Warning: Model key '{model_key}' not found in initialized instances. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    model_name = llm.model_name # Use name from LLM object for consistency\n",
        "    print(f\"\\n===== Evaluating model: {model_name} =====\")\n",
        "\n",
        "    # Paths for temporary files specific to this model iteration\n",
        "    temp_orig_bea_path = None\n",
        "    temp_cor_bea_path = None\n",
        "    temp_hyp_m2_path = None\n",
        "\n",
        "    try:\n",
        "        # --- Create temporary file for ORIGINAL sentences (BEA format) --- #\n",
        "        # This file is needed as -orig input for errant_parallel\n",
        "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".txt\", encoding='utf-8') as temp_orig_file:\n",
        "            for sentence in original_sentences_bea: # Write the BEA format sentences\n",
        "                temp_orig_file.write(sentence + '\\n')\n",
        "            temp_orig_bea_path = temp_orig_file.name\n",
        "        print(f\"Created temp orig (BEA format): {temp_orig_bea_path}\")\n",
        "\n",
        "\n",
        "        # --- Run Model (using detokenized input) --- #\n",
        "        start_time = time.time()\n",
        "        print(f\"Generating corrections for {len(natural_input_sentences)} sentences...\")\n",
        "        # Pass the NATURAL language sentences to the LLM\n",
        "        corrected_sentences_natural = llm(natural_input_sentences)\n",
        "        generation_time = time.time() - start_time\n",
        "        print(f\"Generation took: {generation_time:.2f} seconds\")\n",
        "\n",
        "        # --- Retokenize Output --- #\n",
        "        print(\"Retokenizing LLM output to BEA format...\")\n",
        "        corrected_sentences_bea = [tokenize_like_bea(s) for s in corrected_sentences_natural]\n",
        "        print(\"Retokenization complete.\")\n",
        "\n",
        "\n",
        "        # --- Log Sample Outputs --- #\n",
        "        if LOG_SAMPLE_OUTPUTS > 0:\n",
        "            print(f\"\\n--- Sample Outputs for {model_name} (First {min(LOG_SAMPLE_OUTPUTS, len(original_sentences_bea))}) ---\")\n",
        "            for i in range(min(LOG_SAMPLE_OUTPUTS, len(original_sentences_bea))):\n",
        "                print(f\"Orig (BEA)  [{i+1}]: {original_sentences_bea[i]}\")\n",
        "                print(f\"Input (Nat) [{i+1}]: {natural_input_sentences[i]}\") # Optional: print detokenized input\n",
        "                print(f\"Output (Nat)[{i+1}]: {corrected_sentences_natural[i]}\") # Optional: print natural output\n",
        "                print(f\"Corr (BEA)  [{i+1}]: {corrected_sentences_bea[i]}\")\n",
        "                print(\"---\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "        # --- Evaluation --- #\n",
        "        if len(corrected_sentences_bea) != len(original_sentences_bea):\n",
        "            print(f\"Error: Number of corrected sentences ({len(corrected_sentences_bea)}) does not match original ({len(original_sentences_bea)}) for model {model_name}. Skipping evaluation.\")\n",
        "            results[model_name] = {\"P\": \"Error\", \"R\": \"Error\", \"F0.5\": \"Length Mismatch\", \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": \"N/A\"}\n",
        "            continue # Skip to the next model\n",
        "\n",
        "        eval_start_time = time.time()\n",
        "\n",
        "        # 3. Write RETOKENIZED CORRECTED sentences (BEA format) to a temporary file\n",
        "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".cor.txt\", encoding='utf-8') as temp_cor_file:\n",
        "            for sentence in corrected_sentences_bea: # Write the BEA format sentences\n",
        "                temp_cor_file.write(sentence + '\\n')\n",
        "            temp_cor_bea_path = temp_cor_file.name\n",
        "        print(f\"Created temp corrected (BEA format): {temp_cor_bea_path}\")\n",
        "\n",
        "\n",
        "        # 4. Create hypothesis M2 file using errant_parallel\n",
        "        #    Input: -orig (BEA format), -cor (BEA format)\n",
        "        #    Output: hypothesis M2 (ERRANT will likely re-tokenize internally based on spaCy,\n",
        "        #            but we feed it BEA format for consistency at this step)\n",
        "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".hyp.m2\") as temp_hyp_m2_file:\n",
        "            temp_hyp_m2_path = temp_hyp_m2_file.name\n",
        "\n",
        "        print(f\"Running errant_parallel... Orig: {temp_orig_bea_path}, Cor: {temp_cor_bea_path}, Out: {temp_hyp_m2_path}\")\n",
        "        parallel_cmd = [\"errant_parallel\", \"-orig\", temp_orig_bea_path, \"-cor\", temp_cor_bea_path, \"-out\", temp_hyp_m2_path]\n",
        "        parallel_result = subprocess.run(parallel_cmd, capture_output=True, text=True, check=False)\n",
        "\n",
        "        if parallel_result.returncode != 0:\n",
        "            print(f\"Error running errant_parallel for {model_name}:\")\n",
        "            print(f\"Stderr: {parallel_result.stderr}\")\n",
        "            results[model_name] = {\"P\": \"Error\", \"R\": \"Error\", \"F0.5\": \"errant_parallel failed\", \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": \"N/A\"}\n",
        "            continue # Skip to next model\n",
        "        if \"Processing 0 sentences\" in parallel_result.stderr: # Check if ERRANT processed anything\n",
        "             print(f\"Warning: errant_parallel reported processing 0 sentences. Check input files or ERRANT setup.\")\n",
        "             print(f\"Stderr: {parallel_result.stderr}\")\n",
        "             # Decide whether to continue or mark as error\n",
        "\n",
        "        # 5. Compare hypothesis M2 with reference M2 using errant_compare\n",
        "        #    Input: -hyp (Generated M2), -ref (BEA Reference M2 - potentially subsetted)\n",
        "        print(f\"Running errant_compare... Hyp: {temp_hyp_m2_path}, Ref: {reference_m2_to_use}\")\n",
        "        compare_cmd = [\"errant_compare\", \"-hyp\", temp_hyp_m2_path, \"-ref\", reference_m2_to_use]\n",
        "        compare_result = subprocess.run(compare_cmd, capture_output=True, text=True, check=False)\n",
        "\n",
        "        if compare_result.returncode != 0:\n",
        "            print(f\"Error running errant_compare for {model_name}:\")\n",
        "            print(f\"Stderr: {compare_result.stderr}\")\n",
        "            results[model_name] = {\"P\": \"Error\", \"R\": \"Error\", \"F0.5\": \"errant_compare failed\", \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": \"N/A\"}\n",
        "            continue # Skip to next model\n",
        "\n",
        "        # 6. Parse scores\n",
        "        eval_time = time.time() - eval_start_time\n",
        "        print(f\"Evaluation took: {eval_time:.2f} seconds\")\n",
        "        # print(f\"errant_compare output for {model_name}:\\n{compare_result.stdout}\") # Uncomment for debugging\n",
        "\n",
        "        match = score_pattern.search(compare_result.stdout)\n",
        "        if match:\n",
        "            p, r, f05 = match.groups()\n",
        "            print(f\"Scores for {model_name} (on {len(original_sentences_bea)} samples): P={p}, R={r}, F0.5={f05}\")\n",
        "            results[model_name] = {\n",
        "                \"P\": float(p),\n",
        "                \"R\": float(r),\n",
        "                \"F0.5\": float(f05),\n",
        "                \"Samples\": len(original_sentences_bea),\n",
        "                \"Generation Time (s)\": f\"{generation_time:.2f}\",\n",
        "                \"Eval Time (s)\": f\"{eval_time:.2f}\"\n",
        "            }\n",
        "        else:\n",
        "            print(f\"Could not parse scores from errant_compare output for {model_name}.\")\n",
        "            print(f\"Output:\\n{compare_result.stdout}\")\n",
        "            results[model_name] = {\"P\": \"Error\", \"R\": \"Error\", \"F0.5\": \"Parsing failed\", \"Samples\": len(original_sentences_bea), \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": f\"{eval_time:.2f}\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during evaluation for {model_name}: {e}\")\n",
        "        # Attempt to calculate eval time even if error occurred mid-evaluation\n",
        "        current_eval_time = time.time() - (eval_start_time if 'eval_start_time' in locals() and eval_start_time else start_time)\n",
        "        results[model_name] = {\"P\": \"Error\", \"R\": \"Error\", \"F0.5\": f\"Exception: {type(e).__name__}\", \"Samples\": len(original_sentences_bea), \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": f\"{current_eval_time:.2f}\"}\n",
        "    finally:\n",
        "        # 7. Clean up temporary files created in THIS iteration\n",
        "        for path in [temp_orig_bea_path, temp_cor_bea_path, temp_hyp_m2_path]:\n",
        "            if path and os.path.exists(path):\n",
        "                try:\n",
        "                    os.remove(path)\n",
        "                except OSError as e:\n",
        "                    print(f\"  Error removing {path}: {e}\")\n",
        "\n",
        "# --- Final Cleanup for Subset Reference M2 ---\n",
        "if temp_ref_m2_subset_path and os.path.exists(temp_ref_m2_subset_path):\n",
        "     try:\n",
        "         os.remove(temp_ref_m2_subset_path)\n",
        "     except OSError as e:\n",
        "         print(f\"  Error removing {temp_ref_m2_subset_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SVgMpCSULdxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af451586-50fb-4ab5-e06e-54099c7c8aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Benchmark Results ---\n",
            "Model: gemma-3-4b-it-GGUF\n",
            "  F0.5: 0.3415\n",
            "  P: 0.3289\n",
            "  R: 0.4032\n",
            "  Samples: 25\n",
            "  Generation Time (s): 355.55\n",
            "  Eval Time (s): 13.40\n",
            "--------------------\n",
            "Model: nomodit-4b-merged-v0-Q4_K_M-GGUF\n",
            "  F0.5: 0.4274\n",
            "  P: 0.4651\n",
            "  R: 0.3226\n",
            "  Samples: 25\n",
            "  Generation Time (s): 355.83\n",
            "  Eval Time (s): 9.14\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n--- Benchmark Results ---\")\n",
        "# Sort results alphabetically by model name for consistent output\n",
        "sorted_results = dict(sorted(results.items()))\n",
        "\n",
        "for model_name, scores in sorted_results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    # Define a preferred order for metrics if desired\n",
        "    metric_order = [\"F0.5\", \"P\", \"R\", \"Samples\", \"Generation Time (s)\", \"Eval Time (s)\"]\n",
        "    for metric in metric_order:\n",
        "        if metric in scores:\n",
        "            value = scores[metric]\n",
        "            # Format floats nicely\n",
        "            if isinstance(value, float):\n",
        "                print(f\"  {metric}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {metric}: {value}\")\n",
        "    # Print any other metrics not in the preferred order (e.g., error messages)\n",
        "    for metric, value in scores.items():\n",
        "        if metric not in metric_order:\n",
        "             print(f\"  {metric}: {value}\")\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "STtaG0e4LgQP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8afc733442c64672aed1542bf205ead4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89ffe66699ce4b8c84f7c85253ce5801",
              "IPY_MODEL_af30be4b368d41228b1a6ff51717bc23",
              "IPY_MODEL_a5b7a19ad11e4ce3980a3b743051f98b"
            ],
            "layout": "IPY_MODEL_f536a0b0dd5e49e1bec951de133f82b8"
          }
        },
        "89ffe66699ce4b8c84f7c85253ce5801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8f765546c334ead8d8a76ce69dc8334",
            "placeholder": "​",
            "style": "IPY_MODEL_7eba7751b91c4562a9b02dc9ec5caa32",
            "value": "gemma-3-4b-it-Q4_K_M.gguf: 100%"
          }
        },
        "af30be4b368d41228b1a6ff51717bc23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eea0c9e8f6d5432e8535b52ee9ed84cf",
            "max": 2489894016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0006db3929c34f83aed7a93f89511eaf",
            "value": 2489894016
          }
        },
        "a5b7a19ad11e4ce3980a3b743051f98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ef95ac10f44d2f9bfd190ebd95c71a",
            "placeholder": "​",
            "style": "IPY_MODEL_5de55033ce4841a1a6f885a72520683f",
            "value": " 2.49G/2.49G [00:34&lt;00:00, 71.6MB/s]"
          }
        },
        "f536a0b0dd5e49e1bec951de133f82b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8f765546c334ead8d8a76ce69dc8334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eba7751b91c4562a9b02dc9ec5caa32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eea0c9e8f6d5432e8535b52ee9ed84cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0006db3929c34f83aed7a93f89511eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88ef95ac10f44d2f9bfd190ebd95c71a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5de55033ce4841a1a6f885a72520683f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f0e2595516d4151947b68f26148e9f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f1aa35321784b958668845ad974c41b",
              "IPY_MODEL_4047c897c50243bd8190f858dc2ab369",
              "IPY_MODEL_5e78473f15f547549e43372cba12b528"
            ],
            "layout": "IPY_MODEL_e364935ffc8c4f6086e1203643b3de85"
          }
        },
        "8f1aa35321784b958668845ad974c41b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32bada8fa6d74de7bd7f09216868cab1",
            "placeholder": "​",
            "style": "IPY_MODEL_04f428e97fa84541acd5684239708041",
            "value": "nomodit-4b-merged-v0-q4_k_m.gguf: 100%"
          }
        },
        "4047c897c50243bd8190f858dc2ab369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f708042ba4f34992a377ea678d7283f4",
            "max": 2489894176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1079de77ecb74dbca5e0ba54aff3b414",
            "value": 2489894176
          }
        },
        "5e78473f15f547549e43372cba12b528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fa6a48e43ca46c2ad594b9f9b0c7d11",
            "placeholder": "​",
            "style": "IPY_MODEL_7bcbfa0f22cc4258af9e12e95b823627",
            "value": " 2.49G/2.49G [00:37&lt;00:00, 48.1MB/s]"
          }
        },
        "e364935ffc8c4f6086e1203643b3de85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32bada8fa6d74de7bd7f09216868cab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04f428e97fa84541acd5684239708041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f708042ba4f34992a377ea678d7283f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1079de77ecb74dbca5e0ba54aff3b414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fa6a48e43ca46c2ad594b9f9b0c7d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bcbfa0f22cc4258af9e12e95b823627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}